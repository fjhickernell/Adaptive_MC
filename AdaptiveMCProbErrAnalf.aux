\relax 
\immediate\closeout\minitoc
\let \MiniTOC =N
\@writefile{toc}{\contentsline {title}{Automatic Monte Carlo Algorithms Where the Integrand Size Is Unknown \unskip {}}{1}}
\@writefile{toc}{\contentsline {author}{Fred J. Hickernell\unskip {} \and Lan Jiang\unskip {} \and Yuewei Liu\unskip {} \and Art Owen \unskip {}}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\newlabel{samplemean}{{1}{1}}
\citation{BahSav56}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}The Statistician's Perspective}{2}}
\newlabel{statperspsec}{{1.1}{2}}
\newlabel{abserr}{{2}{2}}
\newlabel{CLT}{{3}{2}}
\newlabel{samplevar}{{4}{2}}
\newlabel{simpleMCest}{{5}{2}}
\newlabel{boundedkurt}{{6}{2}}
\citation{TraWasWoz88}
\citation{MAT7.12}
\citation{GanGau00a}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}The Information-Based Complexity Theorist's or Numerical Analyst's Perspective}{3}}
\newlabel{Lpnormdef}{{7}{3}}
\newlabel{ballintegdef}{{8}{3}}
\newlabel{coneintegdef}{{9}{3}}
\citation{MAT7.12}
\citation{Sha08a}
\citation{TrefEtal12}
\citation{MAT7.12}
\citation{Mat8a}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Illustrative Univariate Examples of Automatic Algorithms}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Plots of fooling functions, $f$, with $\mu =\DOTSI \intop \ilimits@ _0^1 f(x) \tmspace  +\thinmuskip {.1667em} {\rm  d}x=1$, but for which the corresponding algorithms return values of $\mathaccentV {hat}05E{\mu }=0$. }}{4}}
\newlabel{foolfunfig}{{1}{4}}
\newlabel{GaussianTestFun}{{10}{4}}
\newlabel{GaussianTestFun}{{1.3}{5}}
\newlabel{FunPara}{{1.3}{5}}
\newlabel{AlgorPara}{{1.3}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Simple Monte Carlo with Guaranteed Error Estimation}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Reliably Bounding the Variance}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Execution times and errors for test function \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 1.3\hbox {}\unskip \@@italiccorr )}} for $d=1$ and $\varepsilon =10^{-3}$, and a variety of parameters giving a range of $\sigma (f)$ and $\kappa (f)$. The solid line shows that cumulative distribution of actual errors, and the dot-dashed line shows the cumulative distribution of execution times. For the {\tt  cubMC} i.i.d.\ and i.i.d.\ heavy duty the points labeled * are those for which the Theorem 1\hbox {} guarantees the error tolerance. }}{6}}
\newlabel{GaussianTestFunFig}{{2}{6}}
\citation{LinBai10a}
\citation{LinBai10a}
\citation{LinBai10a}
\citation{LinBai10a}
\citation{LinBai10a}
\citation{LinBai10a}
\citation{LinBai10a}
\citation{LinBai10a}
\citation{LinBai10a}
\citation{LinBai10a}
\citation{LinBai10a}
\citation{LinBai10a}
\citation{LinBai10a}
\citation{LinBai10a}
\citation{LinBai10a}
\citation{LinBai10a}
\citation{??}
\newlabel{ChebCantlem}{{1}{7}}
\newlabel{LP}{{1}{7}}
\newlabel{varbdlem}{{2}{7}}
\newlabel{sampvarbd}{{12}{7}}
\newlabel{sampvarup}{{12a}{7}}
\newlabel{sampvarlo}{{12b}{7}}
\newlabel{kappamaxdef}{{13}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces (a) The maximum kurtosis, $\kappa _{\qopname  \relax m{max}}(\alpha ,n_{\sigma },1.5)$, as defined in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 13\hbox {}\unskip \@@italiccorr )}}; (b) comparison of sample sizes $ N_G(0.01,\alpha )$, $N_C(0.01,\alpha )$, and $N_B(0.01,\alpha ,\kappa _{\qopname  \relax m{max}}^{3/4}(\alpha ,1000,1.5))$.}}{8}}
\newlabel{kurtmaxcompareNfig}{{3}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Determining the Sample Size}{8}}
\newlabel{ChebErr}{{14}{8}}
\newlabel{ChebProbEst}{{14a}{8}}
\newlabel{NCdef}{{14b}{8}}
\citation{Pet95a}
\newlabel{BE}{{3}{9}}
\newlabel{BEresult}{{15}{9}}
\newlabel{proberrcritsampleBE}{{16}{9}}
\newlabel{NB}{{16b}{9}}
\newlabel{mainadaptthm}{{1}{9}}
\newlabel{NCBdef}{{17}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Cost of the Algorithm}{10}}
\newlabel{costtheorem}{{2}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {3}More Numerical Examples}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Execution times and errors for test function \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 1.3\hbox {}\unskip \@@italiccorr )}} for $d=7$ and $\varepsilon =10^{-3}$, and a variety of parameters giving a range of $\sigma (f)$ and $\kappa (f)$. The solid line shows that cumulative distribution of actual errors, and the dot-dashed line shows the cumulative distribution of execution times. For the {\tt  cubMC} i.i.d.\ and i.i.d.\ heavy duty the points labeled * are those for which the Theorem 1\hbox {} guarantees the error tolerance.}}{12}}
\newlabel{GausstianTestFunHDFig}{{4}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {4}A General Error Criterion}{12}}
\newlabel{relerrsec}{{4}{12}}
\newlabel{relerrcrit}{{4}{12}}
\newlabel{genrelerrcrit}{{20}{12}}
\newlabel{relerrcritd}{{21}{13}}
\newlabel{NCinv}{{4}{13}}
\newlabel{NBinv*}{{4}{13}}
\newlabel{NCBinv*}{{4}{13}}
\newlabel{alphaseq}{{22}{14}}
\newlabel{alphaseqex}{{23}{14}}
\newlabel{relerradaptthm}{{3}{14}}
\newlabel{boundcstep}{{2}{14}}
\newlabel{newhvarepsstep}{{2c}{14}}
\newlabel{hmufinalstep}{{3}{14}}
\bibstyle{spbasic}
\bibdata{FJH22,FJHown22}
\bibcite{BahSav56}{{1}{1956}{{Bahadur and Savage}}{{}}}
\bibcite{GanGau00a}{{2}{2000}{{Gander and Gautschi}}{{}}}
\bibcite{TrefEtal12}{{3}{2012}{{Hale et~al}}{{Hale, Trefethen, and Driscoll}}}
\bibcite{LinBai10a}{{4}{2010}{{Lin and Bai}}{{}}}
\bibcite{Pet95a}{{5}{1995}{{Petrov}}{{}}}
\bibcite{Sha08a}{{6}{2008}{{Shampine}}{{}}}
\bibcite{MAT7.12}{{7}{2012}{{The MathWorks, Inc.}}{{}}}
\bibcite{TraWasWoz88}{{8}{1988}{{Traub et~al}}{{Traub, Wasilkowski, and Wo\'zniakowski}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{15}}
\@writefile{toc}{\contentsline {section}{References}{15}}
\@mtwritefile{\contentsline {mtchap}{References}{15}}
\bibcite{Mat8a}{{9}{2011}{{Wolfram Research Inc.}}{{}}}
\immediate\closeout\minitoc
